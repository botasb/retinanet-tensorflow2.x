 - [ ] Fix input dataset sharding when training in multi work/ multi host mode. Current implementation skips shuffling dataset to avoid different works processing same sampple. This should be done preferably by setting a global random seed that is shared across all the tpu workers.
 - [ ] refactor post processing
 - [ ] add tf.image.non_max_suppression_padded
 - [ ] enable batch post processing
 - [ ] check saving loading weights and saved_model
 - [ ] make sure model works with existing onnx graphsurgeon and trt conversion scripts
 - [ ] Add support for freezing backbone, fpn, head
 - [ ] Verify fine tuning
 - [ ] Release resnet50 30x schedule model (~39.5-40.5 mAP)z
